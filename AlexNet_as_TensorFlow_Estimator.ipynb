{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AlexNet with `tf.estimator`\n",
    "\n",
    "Not interested in `Keras`? Kick off the training wheels and write it in raw `TensorFlow`, you rebel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Import data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tflearn.datasets.oxflower17 as oxflower17\n",
    "x, y = oxflower17.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make an input function\n",
    "\n",
    "If you're going to use `TensorFlow`, you should get the hang of `tf.data`. It's a little finicky but far easier to use than the old queueing system. And it's incredibly flexible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "split = int(0.7*y.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_input_fn():\n",
    "    def _gen():\n",
    "        for i in range(split):\n",
    "            yield x[i,:,:,:].astype(np.float32), y[i,:].astype(np.int64)\n",
    "    dataset = tf.data.Dataset.from_generator(_gen,\n",
    "                        (tf.float32, tf.int64),\n",
    "                        ([224,224, 3], [17]))\n",
    "    dataset = dataset.shuffle(5000)\n",
    "    dataset = dataset.repeat(1000)\n",
    "    dataset = dataset.batch(128)\n",
    "    dataset = dataset.prefetch(1)\n",
    "    return dataset.make_one_shot_iterator().get_next()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_input_fn():\n",
    "    def _gen():\n",
    "        for i in range(split, y.shape[0]):\n",
    "            yield x[i,:,:,:].astype(np.float32), y[i,:].astype(np.int64)\n",
    "    dataset = tf.data.Dataset.from_generator(_gen,\n",
    "                        (tf.float32, tf.int64),\n",
    "                        ([224,224, 3], [17]))\n",
    "    dataset = dataset.batch(128)\n",
    "    return dataset.make_one_shot_iterator().get_next()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_input_fn()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define a model function\n",
    "\n",
    "Follow the specification here: https://www.tensorflow.org/guide/custom_estimators\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conv_layers = [(96, 11, 4), \n",
    "               \"maxpool\", \n",
    "             (256, 5, 1),\n",
    "              \"maxpool\",\n",
    "              (384, 3, 1),\n",
    "              (384, 3, 1),\n",
    "              (256, 3, 1),\n",
    "              \"maxpool\"\n",
    "             ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_fn(features, labels, mode, params):\n",
    "    is_training = mode == tf.estimator.ModeKeys.TRAIN\n",
    "    \n",
    "    net = tf.identity(features)\n",
    "    \n",
    "    # CONVOLUTIONAL LAYERS\n",
    "    for l in conv_layers:\n",
    "        if l == \"maxpool\":\n",
    "            net = tf.layers.max_pooling2d(net, 3, 2, padding=\"valid\")\n",
    "        else:\n",
    "            f, w, s = l\n",
    "            net = tf.layers.conv2d(net, f, w, strides=(s,s),\n",
    "                                  padding=\"valid\", \n",
    "                                  activation=tf.nn.relu)\n",
    "            net = tf.layers.batch_normalization(net)\n",
    "            \n",
    "    # DENSE LAYERS\n",
    "    net = tf.layers.flatten(net)\n",
    "    for _ in range(2):\n",
    "        net = tf.layers.dense(net, 2048, activation=tf.nn.relu)\n",
    "        net = tf.layers.dropout(net, 0.5, training=is_training)\n",
    "    \n",
    "    # OUTPUT LAYER\n",
    "    logits = tf.layers.dense(net, 17)\n",
    "    \n",
    "    probs = tf.nn.softmax(logits)\n",
    "    predicted_classes = tf.argmax(logits, 1)\n",
    "    \n",
    "    # PREDICT MODE\n",
    "    if mode == tf.estimator.ModeKeys.PREDICT:\n",
    "        predictions = {\n",
    "            \"probabilities\":probs,\n",
    "            \"class_ids\":predicted_classes\n",
    "        }\n",
    "        return tf.estimator.EstimatorSpec(mode, \n",
    "                            predictions=predictions)\n",
    "    # EVALUATE MODE\n",
    "    loss = tf.losses.softmax_cross_entropy(labels, logits)\n",
    "\n",
    "    accuracy = tf.metrics.accuracy(labels=tf.argmax(labels, axis=1), \n",
    "                                  predictions=predicted_classes)\n",
    "    metrics = {\"accuracy\":accuracy}\n",
    "    tf.summary.scalar(\"accuracy\", accuracy[1])\n",
    "    \n",
    "    if mode == tf.estimator.ModeKeys.EVAL:\n",
    "        return tf.estimator.EstimatorSpec(\n",
    "            mode, loss=loss, eval_metric_ops=metrics\n",
    "        )\n",
    "    # TRAIN MODE\n",
    "    optimizer = tf.train.AdamOptimizer(1e-3)\n",
    "    train_op = optimizer.minimize(loss, \n",
    "                    global_step=tf.train.get_global_step())\n",
    "    \n",
    "    return tf.estimator.EstimatorSpec(mode, loss=loss, \n",
    "                                    train_op=train_op)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.estimator.Estimator(\n",
    "    model_fn=model_fn, params={},\n",
    "    model_dir=\"logs\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## train it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(100):\n",
    "    model.train(train_input_fn, steps=250)\n",
    "    model.evaluate(eval_input_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
